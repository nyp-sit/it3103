{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "First CNN for Image Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/session-1/first_cnn_for_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "# First Convolutional Neural Network for Image Classification\n",
        "\n",
        "In this exercise, you will learn to build your first simple Convolutional Neural Network and use it to classify images. \n",
        "\n",
        "You will learn: \n",
        "- how to construct a Convolutional Neural Networks \n",
        "- adjust the different hyper-parameters of the network (e.g. number of filters, number of layers, etc) and observe the effects \n",
        "- how to visualize the activations of the hidden layers \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQmFSPe7Wm8d"
      },
      "source": [
        "## Fashion MNIST Dataset\n",
        "\n",
        "We will be using a toy dataset [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. \n",
        "\n",
        "![fashion-mnist](https://github.com/nyp-sit/sdaai-iti107/blob/main/session-1/images/fashion-mnist.png?raw=1)\n",
        "\n",
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "|Label|Class|\n",
        "|---|---|\n",
        "|0|T-shirt/top|\n",
        "|1|Trouser|\n",
        "|2|Pullover|\n",
        "|3|Dress|\n",
        "|4|Coat|\n",
        "|5|Sandal|\n",
        "|6|Shirt|\n",
        "|7|Sneaker|\n",
        "|8|Bag|\n",
        "|9|Ankle boot|       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3XzIfPkWm8e"
      },
      "source": [
        "Let's load the data using `keras.datasets` as it is part of datasets available from keras.\n",
        "For a list of dataset available from keras, see https://www.tensorflow.org/api_docs/python/tf/keras/datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tFgT1MMKi6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (validation_images, validation_labels) = mnist.load_data()\n",
        "print('Shape of training_images = {}'.format(training_images.shape))\n",
        "print('Shape of validation_images = {}'.format(validation_images.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvK-o6CanZur"
      },
      "source": [
        "Note that the data is in numpy arrays and not tensor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHLv_hwNnZur"
      },
      "source": [
        "print(type(training_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6JrBRwWm8g"
      },
      "source": [
        "## Preprocess the images\n",
        "\n",
        "You need to preprocess the image before using it as the input to the CNN.\n",
        "CNN expects our input to be of the shape (batch, height, width, channels). Furthermore, the pixel values of the original image is in the range (0,255). Neural network will learn better if the input values are normalized to between (0.0, 1.0). \n",
        "\n",
        "**Exercise 1**\n",
        "\n",
        "Reshape and scale both the train and validation images.\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# reshape to a 4-D tensors, with number of channel as 1, since this is a gray scale image\n",
        "training_images = np.expand_dims(training_images, axis=3)\n",
        "validation_images = np.expand_dims(validation_images, axis=3)\n",
        "\n",
        "# scale the input to between 0. and 1.0\n",
        "training_images = training_images / 255.0\n",
        "validation_images = validation_images / 255.0\n",
        "```\n",
        "</details> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MI9zUqGWm8h"
      },
      "source": [
        "# Complete the code \n",
        "import numpy as np\n",
        "\n",
        "# reshape to a 4-D tensors, with number of channel as 1, since this is a gray scale image\n",
        "training_images = ??\n",
        "validation_images = ??\n",
        "\n",
        "# scale the input to between 0. and 1.0\n",
        "training_images = ??\n",
        "validation_images = ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQIfSSNLnZut"
      },
      "source": [
        "assert training_images.shape == (60000, 28, 28, 1)\n",
        "assert validation_images.shape == (10000, 28, 28, 1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om3eCKWbWm8i"
      },
      "source": [
        "## Build your first CNN\n",
        "\n",
        "A typical CNN consists of 1 or more blocks of Conv2D layer followed by MaxPooling2D layer. The 2D array from the last convolutional block will then be flattened into 1D array before feeding into Dense (fully connected) layer for classification. The last layer uses `softmax` to ouput the probabilities of each of the 10-classes. Note that the last layer has to have same number of output units as the number of classes (in our case, we have 10 classes, so we need 10 output units). \n",
        "\n",
        "\n",
        "**Exercise 2:**\n",
        "\n",
        "Construct a convnet that consist of following: \n",
        "- Conv layer with 32 filters of size 3x3, and using 'relu' activation function, followed by Max Pooling layer of pool size 2x2. \n",
        "- Conv layer with 64 filters of size 3x3, and using 'relu' activation function, followed by Max Pooling layer of pool size 2x2. \n",
        "- Flatten the 2D array into 1D\n",
        "- Fully connected layer with 128 neurons, using 'relu' activation function\n",
        "- Fully connected layer with 10 neurons with a softmax function. \n",
        "\n",
        "Use Adam as your optimizer, with a default learning rate and use appropriate loss function (depending on whether your label is one-hot-encoded or not).\n",
        "\n",
        "<details><summary>Click here for answer</summary>\n",
        "\n",
        "```python\n",
        "def make_model(input_shape, num_classes):\n",
        "\n",
        "    inputs = keras.layers.Input(shape=input_shape, name='input')\n",
        "    x = keras.layers.Conv2D(32, 3, activation='relu', name='conv1')(inputs)\n",
        "    x = keras.layers.MaxPooling2D(2, name='pool1')(x)\n",
        "    x = keras.layers.Conv2D(64, 3, activation='relu', name='conv2')(x)\n",
        "    x = keras.layers.MaxPooling2D(2, name='pool2')(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(128, activation='relu', name='dense1')(x)\n",
        "    \n",
        "    if num_classes > 2: \n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    else: \n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "\n",
        "    outputs = keras.layers.Dense(units, activation=activation, name='dense2')(x)\n",
        "    \n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "# call make_model with appropriate argument values (shape and num_classes)\n",
        "model = make_model((28,28,1), 10)\n",
        "\n",
        "# compile your model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "822gPjDAWm8k"
      },
      "source": [
        "### Start your code here ###\n",
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "\n",
        "    # define the input layer with appropriate shape\n",
        "    inputs = ??\n",
        "\n",
        "    x = keras.layers.Conv2D(32, 3, activation='relu', name='conv1')(inputs)\n",
        "    \n",
        "    # add in more layers here as described above\n",
        "    \n",
        "\n",
        "    if num_classes > 2: \n",
        "        activation = '??'\n",
        "        units = ??\n",
        "    else: \n",
        "        activation = '??'\n",
        "        units = ?\n",
        "        \n",
        "    outputs = keras.layers.Dense(units, activation=activation, name='dense2')(x)\n",
        "    \n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# call make_model with appropriate argument values (shape and num_classes)\n",
        "model = make_model((28,28,1), 10)\n",
        "\n",
        "# compile your model\n",
        "model.compile(optimizer='??', loss='??', metrics=['accuracy'])\n",
        "\n",
        "### End your code ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZkj2gTgByXv"
      },
      "source": [
        "Look at the model summary carefully and make sure you understand why the output shape is as shown and also how to calculate the number of parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTtXgCWGB2Mc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGllb4wvWm8l"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Let's first define a convenience method to create a Tensorboard callback to log the training events. We will also create a ModelCheckpoint callback to save the best-performing set of weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn37WijtYJKa"
      },
      "source": [
        "def create_tb_callback(): \n",
        "\n",
        "    import os\n",
        "    \n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "        \n",
        "        import time\n",
        "        \n",
        "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "        \n",
        "        return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eYLfKh2Wm8m"
      },
      "source": [
        "model.fit(training_images, \n",
        "          training_labels, \n",
        "          batch_size=256, \n",
        "          epochs=30,\n",
        "          validation_data=(validation_images, validation_labels),\n",
        "          callbacks=[create_tb_callback(), model_checkpoint_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAKh85YI8ima"
      },
      "source": [
        "model.load_weights('best_checkpoint')\n",
        "model.evaluate(validation_images, validation_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo9XVvMPWm8m"
      },
      "source": [
        "## Visualize the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSR5hMLsWm8n"
      },
      "source": [
        "%load_ext tensorboard \n",
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oqAEB5uWm8n"
      },
      "source": [
        "We can see that model achieves training accuracy of 98% but the validation accuray stagnates at 92%. So there is some overfitting here. You can try to improve the model by adding in some regularization such as Dropout layer, etc. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "## Visualizing the Convolutions and Pooling\n",
        "\n",
        "It is often said that deep learning network is a blackbox. However, this is certainly not true for Convnets. The representations learnt by Convnets are highly interpretable, as they are representations of visual concepts. \n",
        "\n",
        "The following codes allows us to visualize the output of the feature maps learnt by Convnet. By looking at output (activations) of these feature maps, for different kind of images, we will understand how a specific image is being classified. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daeA1dblWm8o"
      },
      "source": [
        "Let's first print out the labels of the first 10 test labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwWN7jA5Wm8o"
      },
      "source": [
        "print(validation_labels[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMvVVghVWm8p"
      },
      "source": [
        "Let us look two different images, image 0 with label 9 (ankle boot) and image 2 with label 1 (trouser)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6nX4QsOku6"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "ANKLE_BOOT_IDX = 0\n",
        "TROUSER_IDX = 2\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "ax1.imshow(validation_images[ANKLE_BOOT_IDX].reshape(28,28))\n",
        "ax2.imshow(validation_images[TROUSER_IDX].reshape(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yib6Ral5Wm8p"
      },
      "source": [
        "Let's create activation model for each individual layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0q7lfUOWm8q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "import pprint\n",
        "\n",
        "# extract the outputs of layer 1 to  layer 5 (only the Conv2D, MaxPooling2D layers)\n",
        "layer_outputs = [layer.output for layer in model.layers][1:5]\n",
        "pprint.pprint(layer_outputs)\n",
        "\n",
        "# create activation models that will return these outputs given the model input\n",
        "activation_model_conv1 = Model(inputs=model.input, outputs=layer_outputs[0])\n",
        "activation_model_pool1 = Model(inputs=model.input, outputs=layer_outputs[1])\n",
        "activation_model_conv2 = Model(inputs=model.input, outputs=layer_outputs[2])\n",
        "activation_model_pool2 = Model(inputs=model.input, outputs=layer_outputs[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytiBpiOWm8q"
      },
      "source": [
        "Let's look at activations from the 1st Conv2D layer for both images. There are 32 filter maps from the 1st Conv layer, but we going to look at only the first 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoMaVz5Wm8q"
      },
      "source": [
        "fig, axarr = plt.subplots(2, 10, figsize=(20, 4))\n",
        "ankle_boot_activations_conv1 = activation_model_conv1.predict(validation_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
        "trouser_activations_conv1 = activation_model_conv1.predict(validation_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
        "\n",
        "for filter_idx in range(0, 10):\n",
        "    axarr[0, filter_idx].imshow(ankle_boot_activations_conv1[0,:,:, filter_idx])\n",
        "    axarr[1, filter_idx].imshow(trouser_activations_conv1[0,:,:,filter_idx])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ADN_PUWm8r"
      },
      "source": [
        "From the plots, we can see that 1st Conv layer seems to act as detector of lines and edges. Some filters act more like vertical line detectors, whereas some filters detect edges of the shape.\n",
        "\n",
        "Your filter output may not be the same as we have shown here as the specific filters learnt by the Conv layer are not deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izo0O4hHWm8r"
      },
      "source": [
        "Now let's examine the activations from the 2nd Convolutional layer. Again we will only display the output from the first 10 filters.\n",
        "\n",
        "You will observe that the outputs seems to be more abstract and seems to detect a higher-level construct, such a the presence of certain part of the object (e.g. the collar part of the boot)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQzcDmZ2Wm8r"
      },
      "source": [
        "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
        "\n",
        "ankle_boot_activations_conv2 = activation_model_conv2.predict(validation_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
        "trouser_activations_conv2 = activation_model_conv2.predict(validation_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
        "\n",
        "for filter_idx in range(0, 10):\n",
        "    axarr[0, filter_idx].imshow(ankle_boot_activations_conv2[0,:,:, filter_idx])\n",
        "    axarr[1, filter_idx].imshow(trouser_activations_conv2[0,:,:,filter_idx])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6y26nDnWm8s"
      },
      "source": [
        "Now, let's examine the activations from the last max-pooling layer for both images. We will just display the first 10.  What do you observe?\n",
        "\n",
        "The MaxPooling2D just highlight or emphasize more sharply the abstract part detected by the Conv layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRY3v4GcWm8s"
      },
      "source": [
        "fig, axarr = plt.subplots(2,10, figsize=(20,4))\n",
        "\n",
        "ankle_boot_activations_pool2 = activation_model_pool2.predict(validation_images[ANKLE_BOOT_IDX].reshape(1, 28, 28, 1))\n",
        "trouser_activations_pool2 = activation_model_pool2.predict(validation_images[TROUSER_IDX].reshape(1, 28, 28, 1))\n",
        "\n",
        "for filter_idx in range(0, 10):\n",
        "    axarr[0, filter_idx].imshow(ankle_boot_activations_pool2[0,:,:, filter_idx])\n",
        "    axarr[1, filter_idx].imshow(trouser_activations_pool2[0,:,:,filter_idx])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}