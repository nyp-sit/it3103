{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "bert-finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week12/bert-multiclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaeZuPBfwya"
      },
      "source": [
        "# Fine-tuning BERT for Multi-Class Classification (Solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqmw-ZHoBw1I"
      },
      "source": [
        "### Install Hugging Face Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVZlas4f4NK"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVC0VH5Rfwyd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfUyhxsfwye"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di5Bf6u2fwye"
      },
      "source": [
        "data_url = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/news.csv'\n",
        "df = pd.read_csv(data_url, delimiter='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaSt4rGxFSje"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "1en25PLEnDCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeFtqmAEfwyf"
      },
      "source": [
        "SUBSET_SIZE = 2500\n",
        "\n",
        "subset_df = df.sample(n=SUBSET_SIZE, random_state=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df['CATEGORY'].value_counts()"
      ],
      "metadata": {
        "id": "CJ4-4x1ynoLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_label(x):\n",
        "    if x == 'e':\n",
        "        return 0\n",
        "    elif x == 't':\n",
        "        return 1\n",
        "    elif x == 'b':\n",
        "        return 2\n",
        "    elif x == 'm':\n",
        "        return 3\n",
        "\n",
        "labels_map = ['entertainment','tech','business','medical/health']"
      ],
      "metadata": {
        "id": "0O8b5icfoEHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CFHXvtxFZJW"
      },
      "source": [
        "We now convert the text label into numeric values of 0 (negative) and 1 (positive) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODwiiebLfwyf"
      },
      "source": [
        "subset_df['CATEGORY'] =  subset_df['CATEGORY'].apply(map_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzBCEG1efwyg"
      },
      "source": [
        "texts = subset_df['TITLE']\n",
        "labels = subset_df['CATEGORY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQnWEA2fwyh"
      },
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.2)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_texts)"
      ],
      "metadata": {
        "id": "lSc66gS_pKFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM49b__4fwyh"
      },
      "source": [
        "## Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd492GPqfwyh"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFDiYuDTfwyj"
      },
      "source": [
        "train_texts = train_texts.to_list()\n",
        "train_labels = train_labels.to_list()\n",
        "val_texts = val_texts.to_list()\n",
        "val_labels = val_labels.to_list()\n",
        "test_texts = test_texts.to_list()\n",
        "test_labels = test_labels.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23BuebaNfwyj"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJBnhuXfwyk"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZWzPjzQfwyk"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHuehIKfwym"
      },
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\", num_labels=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Since our dataset is already batched, we can simply take the len.\n",
        "num_train_steps = len(train_dataset) * num_epochs\n",
        "\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "ncGcS3E-5O_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "opt = Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "hevZRkXZ6Fc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "lD-dk_ag-J8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8obURu4fwyp"
      },
      "source": [
        "model.save_pretrained('multiclass_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMSvuVBfwyp"
      },
      "source": [
        "## Try out the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyhCrkIALrs9"
      },
      "source": [
        "my_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"multiclass_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input('Write your news article here:')"
      ],
      "metadata": {
        "id": "7gjTRs-07hdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeBzO0YMfwyq"
      },
      "source": [
        "inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "output = my_model(inputs)\n",
        "pred_prob = tf.nn.softmax(output.logits, axis=-1)\n",
        "pred = np.argmax(pred_prob)\n",
        "print(labels_map[pred])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
