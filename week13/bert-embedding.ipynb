{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "bert-embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-8/bert-embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKanFNbpTfv4"
      },
      "source": [
        "# Using BERT as Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8phFkS4Tfv7"
      },
      "source": [
        "Other than fine-tuning BERT for downstream task such as text classification, we can use pretrained BERT model as a feature extractor, very much the same as we are using pretrained CNN such as ResNet as feature extractors for downstream task such as image classification and object detection.  \n",
        "\n",
        "In this lab, we will see how we use a pretrained DistilBert Model to extract features (or embedding) from text and use the extracted features (embeddings) to train a classifier to classify text. You can contrast this with the other lab where we train the DistilBert end to end for the classification, and compare the performance of both. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- extract text embeddings from the bert model \n",
        "- use the extracted features for text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nte0QWOZwOAw"
      },
      "source": [
        "## Install Hugging Face Transformers library\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcAO5A0oVMOj"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiiqcrhLTfv8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh7XepZEZ3Ll"
      },
      "source": [
        "# downloaded the datasets.\n",
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_data_url)\n",
        "test_df = pd.read_csv(test_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD78Yn9zw5Yw"
      },
      "source": [
        "The train set has 40000 samples. We will a small subset (e.g. 2000) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model. We use dataframe's sample() to randomly select a subset of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGpIN8KExCOH"
      },
      "source": [
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE, random_state=128)\n",
        "test_df = test_df.sample(n=TEST_SIZE, random_state=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcdskhQxIcH"
      },
      "source": [
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ORi78_VxN3f"
      },
      "source": [
        "train_texts = train_df['review'].to_list()\n",
        "train_labels = train_df['sentiment'].to_list()\n",
        "test_texts = test_df['review'].to_list()\n",
        "test_labels = test_df['sentiment'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ikfIcafxSMC"
      },
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmboqVJWTfwA"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-uncased\".  This is the same as the other lab exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5THnkPITfwA"
      },
      "source": [
        "from transformers import AutoTokenizer \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ltReujTfwA"
      },
      "source": [
        "The pretrained DistilBERT [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) expects a string or list of string, so we need to convert the data frame (or series) into list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFbiO3STfwB"
      },
      "source": [
        "Here we will tokenize the text string, and pad the text string to the longest sequence in the batch, and also to truncate the sequence if it exceeds the maximum length allowed by the model (in BERT's case, it is 512)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnqjMMCWTfwB"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgdY5C9TfwB"
      },
      "source": [
        "We will create a tensorflow dataset and use it's efficient batching later to obtain the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Znx8EnTfwB"
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HNwF7yYTfwB"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBuGjl7fTfwC"
      },
      "source": [
        "Here we instantiate a pretrained model from 'distilbert-base-cased' and specify output_hidden_state=True so that we get the output from each of the attention layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Pgl3CDTfwC"
      },
      "source": [
        "## Feature Extraction using (Distil)BERT. \n",
        "\n",
        "Here we will load the pretrained model for distibert-based-uncased and use it to extract features from the text (i.e. emeddings). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKSzCCRyTfwC"
      },
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\",output_hidden_states=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrWYnDBTfwD"
      },
      "source": [
        "The model will produce two outputs: the 1st output `output[0]` is of shape `(16, 512, 768)` which corresponds to the output of the last hidden layer and the second output `output[1]` is a list of 7 outputs of shape `(16, 512, 768)`, corresponding to the output of each of the 6 attention layers and the output. 768 refers to the hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B793BJtwTfwD"
      },
      "source": [
        "def extract_features(dataset):\n",
        "\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    for encoding, label in dataset:\n",
        "        output = model(encoding)\n",
        "        hidden_states = output[1]\n",
        "        # here we take the output of the second last attention layer as our embeddings. \n",
        "        # We take the average of the embedding value of 512 tokens (at axis=1) to generate sentence embedding  \n",
        "        sentence_embedding = tf.reduce_mean(hidden_states[-2], axis=1).numpy()\n",
        "        embeddings.append(sentence_embedding)\n",
        "        labels.append(label)\n",
        "    \n",
        "    embeddings, labels = np.concatenate(embeddings), np.concatenate(labels)\n",
        "\n",
        "    return embeddings, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5i4L7XTfwD"
      },
      "source": [
        "X_train, y_train = extract_features(train_dataset)\n",
        "X_val, y_val = extract_features(val_dataset)\n",
        "X_test, y_test = extract_features(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhAG0WnTfwD"
      },
      "source": [
        "## Train a classifier using the extracted features (embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiA0JMdzTfwE"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQhBq-1sTfwE"
      },
      "source": [
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train score : {clf.score(X_train, y_train)}')\n",
        "print(f'validation score : {clf.score(X_val, y_val)}')\n",
        "print(f'test score : {clf.score(X_test, y_test)}')"
      ],
      "metadata": {
        "id": "wOU0lubfFS6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyaH5WnxTfwE"
      },
      "source": [
        "We should be getting an validation and accuracy score of around 86% to 87% which is quite good, considering we are training with only 2000 samples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVDxRUdTfwE"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "1. Modify the code to use the hidden states from a different attention layer as features or take average of hidden states  from few layers as features. \n",
        "2. Modify the code to use BERT model and see if it performs better than the DistilBERT. For BERT Model, the output of different layers are in `output[2]`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ohs_qdbrthxv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}