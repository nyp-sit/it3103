{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "bert-finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-7/bert-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaeZuPBfwya"
      },
      "source": [
        "# Fine-tuning BERT for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrE6epjAfwyd"
      },
      "source": [
        "One of the approaches where we can use BERT for downstream task such as text classification is to do fine-tuning of the pretrained model. \n",
        "\n",
        "In this lab, we will see how we can use a pretrained DistilBert Model and fine-tune it with custom training data for text classification task. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- configure the transformer model for fine-tuning \n",
        "- train the model for binary and multi-class text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqmw-ZHoBw1I"
      },
      "source": [
        "### Install Hugging Face Transformers library\n",
        "\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVZlas4f4NK"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVC0VH5Rfwyd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfUyhxsfwye"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di5Bf6u2fwye"
      },
      "source": [
        "test_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv'\n",
        "train_data_url = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNjl2iy8fwyf"
      },
      "source": [
        "train_df = pd.read_csv(train_data_url)\n",
        "test_df = pd.read_csv(test_data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaSt4rGxFSje"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhcDPN0wCsWi"
      },
      "source": [
        "The train set has 40000 samples. We will use only a small subset (e.g. 2000) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model.  We use dataframe's `sample()` to randomly select a subset of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeFtqmAEfwyf"
      },
      "source": [
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE)\n",
        "test_df = test_df.sample(n=TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CFHXvtxFZJW"
      },
      "source": [
        "We now convert the text label into numeric values of 0 (negative) and 1 (positive) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODwiiebLfwyf"
      },
      "source": [
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d1tQUKNfwyg"
      },
      "source": [
        "train_df.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzBCEG1efwyg"
      },
      "source": [
        "train_texts = train_df['review']\n",
        "train_labels = train_df['sentiment']\n",
        "test_texts = test_df['review']\n",
        "test_labels = test_df['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQnWEA2fwyh"
      },
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM49b__4fwyh"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-uncased\".  The tokenizer helps to produce the input tokens that are suitable to be used by the DistilBert model, e.g. it automatically append the \\[CLS\\] token in the front of the sequence of tokens and the \\[SEP\\] token at the end of the sequence of tokens , and also the attention mask for those padded positions in the input sequence of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd492GPqfwyh"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-wLa1yfwyh"
      },
      "source": [
        "The DistilBERT tokenizer (identical to Bert tokenizer) use WordPiece vocabulary. It has close to 30000 words. Each word has its own ids, we would need to map the tokens to those ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFAN7yoZfwyi"
      },
      "source": [
        "print(f\"Tokenizer vocab size = {tokenizer.vocab_size}\")\n",
        "print(list(tokenizer.vocab.keys())[6000:6020])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOs5f0Idfwyi"
      },
      "source": [
        "Let us take a closer look at the output of the tokenization process. \n",
        "\n",
        "We notice that the tokenizer will return a dictionary of two items 'input_ids' and 'attention_mask'. The input_ids contains the IDs of the tokens. While the 'attention_mask' contains the masking pattern for those padded positions. If you are using BERT tokenizer, there will be additional item called 'token_type_ids'.\n",
        "\n",
        "We also notice that for the example sentence, the word 'Transformer' is being broken up into two tokens 'Trans' and '##former'. The '##' means that the rest of the token should be attached to the previous one.\n",
        "\n",
        "We also see that the tokenizer appended \\[CLS\\] to the beginning of the token sequence, and \\[SEP\\] at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS2hQw8ifwyi"
      },
      "source": [
        "test_sentence = \"Transformer is really good for Natural Language Processing.\"\n",
        "\n",
        "encoding = tokenizer(test_sentence, padding=True, truncation=True)\n",
        "print(f\"Encoding keys:  {encoding.keys()}\\n\")\n",
        "\n",
        "print(f\"token ids: {encoding['input_ids']}\\n\")\n",
        "print(f\"attention_mask: {encoding['attention_mask']}\\n\")\n",
        "print(f\"tokens: {tokenizer.convert_ids_to_tokens(encoding['input_ids'])}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_npR61rfwyj"
      },
      "source": [
        "Now let's go ahead and tokenize our texts. But before we do so, we need to convert the pandas series to list first as the tokenizer cannot work with pandas series or dataframe directly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFDiYuDTfwyj"
      },
      "source": [
        "train_texts = train_texts.to_list()\n",
        "train_labels = train_labels.to_list()\n",
        "val_texts = val_texts.to_list()\n",
        "val_labels = val_labels.to_list()\n",
        "test_texts = test_texts.to_list()\n",
        "test_labels = test_labels.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23BuebaNfwyj"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygaYFdjfwyk"
      },
      "source": [
        "We then create a tensorflow dataset using the encodings and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJBnhuXfwyk"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZWzPjzQfwyk"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi4-umnlfwyl"
      },
      "source": [
        "Now let us fine-tune the pre-trained model by training it with our custom dataset.  \n",
        "\n",
        "We will instantiate a pretrained model 'distilbert-base-uncased', using `TFAutoModelForSequenceClassification`, and passing `num_labels=2` to indicate we want to train a 2-class (binary) classifier.\n",
        "\n",
        "The model is a `tf.keras.Model` subclass. So you can train the model using Keras API such as `fit()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHuehIKfwym"
      },
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models benefit from a much lower learning rate than the default used by Adam, which is 1e-3. In this training, we will start the training with 5e-5 (0.00005) and slowly reduce the learning rate over the course of training. In the literature, you will sometimes see this referred to as decaying or annealing the learning rate. In Keras, the best way to do this is to use a learning rate scheduler. A good one to use is PolynomialDecay. Despite the name, with default settings it simply linearly decays the learning rate from the initial value to the final value over the course of training, which is exactly what we want. In order to use a scheduler correctly, though, we need to tell it how long training is going to be. We compute that as `num_train_steps` below."
      ],
      "metadata": {
        "id": "ex2Kpm8d4G1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Since our dataset is already batched, we can simply take the len.\n",
        "num_train_steps = len(train_dataset) * num_epochs\n",
        "\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "ncGcS3E-5O_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwCWbjeqfwyl"
      },
      "source": [
        "Now we will just compile the model with the learning rate scheduler and the loss function and train our model for 1 epoch. \n",
        "\n",
        "Note that the transformer model output logits directly instead of going through a softmax layer. In your loss function, you will need to set `from_logits=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "opt = Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "hevZRkXZ6Fc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RpchDiofwyn"
      },
      "source": [
        "You will notice that validation accuracy reaches around 89%.  Let's evaluate on our test set. We should see around the same accuracy. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "lD-dk_ag-J8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just go ahead and save our model for inference later. Note that we use transformers library specific save method `save_pretrained()` instead of normal keras model save."
      ],
      "metadata": {
        "id": "z0LZ-L3H-VDG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8obURu4fwyp"
      },
      "source": [
        "model.save_pretrained('finetuned_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMSvuVBfwyp"
      },
      "source": [
        "## Try out the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZXWuxUufwyp"
      },
      "source": [
        "Now let's try out our model with our own sentence.  We first load our saved fined-tuned model using `from_pretrained()` method and specify the folder name where we saved the model to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyhCrkIALrs9"
      },
      "source": [
        "my_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"finetuned_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input('Write your review here:')"
      ],
      "metadata": {
        "id": "7gjTRs-07hdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeBzO0YMfwyq"
      },
      "source": [
        "inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "output = my_model(inputs)\n",
        "pred_prob = tf.nn.softmax(output.logits, axis=-1)\n",
        "print(pred_prob)\n",
        "pred = np.argmax(pred_prob)\n",
        "print(pred)\n",
        "if pred == 1:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh0X1-Dtfwyq"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "- Try to use DistilBERT base-cased pretrained model (or BERT base-uncased) and see if you get better or worse performance. \n",
        "- Try multi-class classification using the this [dataset](https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/news.csv) that groups news title into 4 categories: e (entertainment), b (business), t (tech), m (medical/health). Original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)"
      ]
    }
  ]
}