{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers, what can they do?",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107/blob/main/session-7/transformer_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el1cqlSpTcWe"
      },
      "source": [
        "# A Quick Tour of What Transformers can do?\n",
        "\n",
        "In this exercise, we will use Hugging Face ðŸ¤— Transformer library to perform some common NLP tasks. \n",
        "\n",
        "We will use the pipeline() function, which supports several NLP tasks such as classification, summarization, machine translation and so on. For a list of support tasks see the documentation [here](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline). Pipeline connects a task-specific model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWsvPM7TcWg"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IfOjzLPTcWh"
      },
      "source": [
        "# install the extra package \"sentencepiece\" required for machine translation tasks\n",
        "!pip install datasets transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnczDwT-b9cp"
      },
      "source": [
        "### Sentiment Analysis \n",
        "\n",
        "This pipeline uses the default model fine-tuned on Stanford Sentiment Treebank v2 dataset - \"distilbert-base-uncased-finetuned-sst-2-english\" to classify if a text express positive or negative sentiment. \n",
        "\n",
        "If you want to use other models available from Hugging Face models library, you can specify it in the parameter `pipeline(\"sentiment-analysis\", model=???)`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RllwJ1cdTcWi"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I fell asleep during the movie.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmNqgbWbelAF"
      },
      "source": [
        "### Zero-shot classification \n",
        "\n",
        "Zero-shot-classification pipeline allows you to specify which labels to use for the classification, so you donâ€™t have to rely on the labels of the pretrained model. You can classify the text using the labels you specified. \n",
        "\n",
        "This is especially useful for real-world projects where you have a lot of unlabelled data and do not have the resources to annotate your data. Zero-shot classification allows you to quickly annotate your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWQTitzLTcWj"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"The CO2 emission has been growing at an alarming rate.\",\n",
        "    candidate_labels=[\"environment\", \"politics\", \"business\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gk1zo84gCoC"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "Now letâ€™s see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so itâ€™s normal if you donâ€™t get the same results as shown below. The default model used is gpt-2.\n",
        "\n",
        "You can control how many different sequences are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZylpuzgTcWk"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"Harry Potter whipped out his wand and\", max_length=50, num_return_sequences=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZLHEFBoqwyX"
      },
      "source": [
        "**Exercise 1**\n",
        "\n",
        "Try generating text in another language. \n",
        "\n",
        "Go to the [Hugging Face Model Hub](https://huggingface.co/models) and click on the corresponding tag on the left to display only the supported models for text generation task. You can then refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfx7cu_yoQd4"
      },
      "source": [
        "### Mask filling\n",
        "\n",
        "The next pipeline youâ€™ll try is fill-mask. The idea of this task is to fill in the blanks in a given text. \n",
        "\n",
        "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special <mask> word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so itâ€™s always good to verify the proper mask word when exploring other models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH0gRd3vTcWl"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"The tech giant has been accused of trademark <mask> by other companies.\", top_k=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4MrbU1nt_sn"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. For example, `Michael Bloomberg` was the ex-mayor of `New York`. Michael Bloomberg will be identified as PER, whereas New York will be identified as LOC. \n",
        "\n",
        "We pass the option `grouped_entities=True` in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity, e.g. \"Michael\" and \"Bloomberg\" are parts that refer to the same person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrEVHBKTTcWm"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"Lee Kuan Yew previously lived at 38 Oxley Road.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euYWzKg3wPBg"
      },
      "source": [
        "### Question Answering\n",
        "\n",
        "The question-answering pipeline answers questions using information from a given context. Note that the answer is extracted from the given context and not generated. The `start` and `end` in the example below tells you the span of the text in the context that provide the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSgyTlnRTcWm"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"What course I am studying?\",\n",
        "    context=\"I am currently studying part time in NYP, taking a course in Specialist Diploma in Applied AI.\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym_9-2BhxjIn"
      },
      "source": [
        "### Summarization\n",
        "\n",
        "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Like with text generation, you can specify a `max_length` or a `min_length` for the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuBDVgJ-TcWn"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of \n",
        "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
        "    the premier American universities engineering curricula now concentrate on \n",
        "    and encourage largely the study of engineering science. As a result, there \n",
        "    are declining offerings in engineering subjects dealing with infrastructure, \n",
        "    the environment, and related issues, and greater concentration on high \n",
        "    technology subjects, largely supporting increasingly complex scientific \n",
        "    developments. While the latter is important, it should not be at the expense \n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other \n",
        "    industrial countries in Europe and Asia, continue to encourage and advance \n",
        "    the teaching of engineering. Both China and India, respectively, graduate \n",
        "    six and eight times as many traditional engineers as does the United States. \n",
        "    Other industrial countries at minimum maintain their output, while America \n",
        "    suffers an increasingly serious decline in the number of engineering graduates \n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiajlXFFx35s"
      },
      "source": [
        "### Translation\n",
        "\n",
        "You can choose the model that corresponds to the language pair you want to translate. For example, if you want to translate from fr to en, you need to choose the model that has a naming like \"mt-fr-en\".  Similary, if I want to translate from English to Chinese, then it should be \"mt-en-zh\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xC6xIXtTcWn"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
        "translator(\"US government has been slow in responding to the threat of pandemic.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}