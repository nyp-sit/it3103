{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "intent_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week14/intent_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTTt2on1Kdr-"
      },
      "source": [
        "# Intent Recognition\n",
        "\n",
        "In this practical, we will learn how to apply the HuggingFace Transformers library to our own Intent Recognition task for our chatbot.\n",
        "\n",
        "####**NOTE: Be sure to set your runtime to a GPU instance!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prPHVBopKx1O"
      },
      "source": [
        "## Install the Hugging Face Transformers Library\n",
        "\n",
        "Run the following cell below to install the transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXufUo5E0mfK"
      },
      "source": [
        "!pip install transformers==4.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eyUK96X91ud"
      },
      "source": [
        "## Getting the data and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX2EMQRpl5kA"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_url = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/airchat_intents.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbViUOJL_B5m"
      },
      "source": [
        "We noticed that there are two columns 'Label' and 'Text'. Let's just examine what are the different labels we have and how many samples we have for each labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flTF4DB-_Qee"
      },
      "source": [
        "df['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCOUBlEr_cJ1"
      },
      "source": [
        "We can see that some labels have very few sample such as 'atis_meal', 'atis_airline#atis_flight_no', 'atis_cheapest', and so on. We so few samples, our model will have difficulty in learning any meaningful pattern from it. We will group these labels (with few samples) into a new label called 'others'.  \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr6YP12kwOyD"
      },
      "source": [
        "### Re-define our Classification Labels\n",
        "\n",
        "Here we define the labels we are interested in classifying based on the original labels, and also we added a new label called 'Others'.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdoYqzUQvnRt"
      },
      "source": [
        "# Create a list of unique labels that we will recognize.\n",
        "#\n",
        "sentence_labels = [\n",
        "              \"others\",\n",
        "              \"atis_abbreviation\",\n",
        "              \"atis_aircraft\",\n",
        "              \"atis_airfare\",\n",
        "              \"atis_airline\",\n",
        "              \"atis_flight\",\n",
        "              \"atis_flight_time\",\n",
        "              \"atis_greeting\",\n",
        "              \"atis_ground_service\",\n",
        "              \"atis_quantity\",\n",
        "              \"atis_yes\",\n",
        "              \"atis_no\"]\n",
        "\n",
        "# This creates a reverse mapping dictionary of \"label\" -> index.\n",
        "# \n",
        "sentence_labels_id_by_label = dict((t, i) for i, t in enumerate(sentence_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51O9M5_4Aao3"
      },
      "source": [
        "Now we will map the previous labels to the few ones we specified in the cell above. We will also convert the text labels into numeric labels (e.g. others->0, atis_abbreviation->1, etc). We can use the `map()` function in dataframe to help us do that. We define a lambda function that do the mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmQkgwklnQ41"
      },
      "source": [
        "df['Label'] = df['Label'].map(lambda label: \n",
        "                              sentence_labels_id_by_label[label] \n",
        "                              if label in sentence_labels_id_by_label \n",
        "                              else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7KmBHB3A-DP"
      },
      "source": [
        "# examine a few random samples \n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXn8KmHAxc1l"
      },
      "source": [
        "### Split Our Data\n",
        "\n",
        "We will now separate the texts and labels and call them all_texts and all_labels and we will split the dataset into training and validation set. We do a stratified split to ensure we have equal representation of different labels in both train and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLTOzdnKB2Us"
      },
      "source": [
        "all_texts = df['Text']\n",
        "all_labels = df['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVyqTUPKxdOR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, \n",
        "                                                                    all_labels, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgKGYupJo59k"
      },
      "source": [
        "train_labels.value_counts()/len(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UpZf9s2o_Xx"
      },
      "source": [
        "val_labels.value_counts()/len(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "milh-Q2-yZBe"
      },
      "source": [
        "### Tokenize the text \n",
        "\n",
        "Before we can use the text for classification, we need to tokenize them. We will use Tokenizer of the pretrained model 'distilbert-base-uncased' as we will be fine-tunining on a pretrained model 'distilbert-base-uncased'. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_8gIcR8yZJl"
      },
      "source": [
        "## before we can feed the texts to tokenizer, we need to convert our texts into list of text string instead of \n",
        "## panda Series. We can do this by using to_list(). \n",
        "\n",
        "train_texts = train_texts.to_list()\n",
        "train_labels = train_labels.to_list()\n",
        "val_texts = val_texts.to_list()\n",
        "val_labels = val_labels.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEDfpTUZqGBK"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFpKNCpLp5n2"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsO-EGFIDs7Z"
      },
      "source": [
        "Once we have the encodings, we will go ahead and create a tensorflow dataset, ready to be used to train our model. Since the HuggingFace pretrained model (the tensorflow version) is a Keras model, it can consume the tf.data dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH9Bv9SKp9e2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmAO-RI8y8fX"
      },
      "source": [
        "## Train Your Sentence Classification Model\n",
        "\n",
        "Run the following cell to download the \"distilbert-base-uncased\" and perform fine-tuning training using the dataset that we have above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLYJMR5LGcD7"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"acc\": (preds == p.label_ids).mean()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDUih8mQAb2X"
      },
      "source": [
        "from transformers import TFAutoModelForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./checkpoints',          # output directory\n",
        "    num_train_epochs=2,                  # total number of training epochs\n",
        "    logging_dir='./tblogs',              # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch'         # run evaluation on validation after every epoch\n",
        ")\n",
        "\n",
        "# we need to use the strategy.scope() here to avoid error saying the weights were trained using different scope\n",
        "# the numb_labels tells the classification model how many outputs we need. It will be as many unique labels we have\n",
        "with training_args.strategy.scope():\n",
        "    intent_classification_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
        "                                                                                     num_labels=len(sentence_labels))\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=intent_classification_model,   # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T1kDV3zrgfG"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYuEFgkfGH8c"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir tblogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT_ucCQv0FSG"
      },
      "source": [
        "### Saving the Model\n",
        "\n",
        "When you training has completed, run the following cell to save your model.\n",
        "\n",
        "Remember to download the model from Google Colab if you want to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eV7zUC-A1Bu"
      },
      "source": [
        "# Save the model\n",
        "#\n",
        "#intent_classification_model.save(\"intentclassification_model\")\n",
        "intent_classification_model.save_pretrained(\"intent_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W60-n-I0TNX"
      },
      "source": [
        "### Evaluating the Model\n",
        "\n",
        "Run the following code to perform interference for the entire training and validation data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbLVVeCFuNku"
      },
      "source": [
        "preds = trainer.predict(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZw7jZauXLM"
      },
      "source": [
        "tf_predictions = tf.nn.softmax(preds.predictions, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THU580l9ucUg"
      },
      "source": [
        "y_preds = np.argmax(tf_predictions, axis=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUt2HCZOufZN"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(preds.label_ids, y_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9T_A7_F3OrR"
      },
      "source": [
        "## Putting Our Model to the Test\n",
        "\n",
        "Run the following cell to create the necessary classes and functions to load our model and perform inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dq45oOFH9Sc"
      },
      "source": [
        "# Import the necessary libraries\n",
        "#\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForSequenceClassification\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Create the DistilBERT tokenizer\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define a function to perform inference on a single input text.\n",
        "# \n",
        "def infer_intent(model, text):\n",
        "    # Passes the text into the tokenizer\n",
        "    #\n",
        "    input = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    print(input)\n",
        "\n",
        "    # Sends the result from the tokenizer into our classification model\n",
        "    #\n",
        "    output = model(input)\n",
        "\n",
        "    # Extract the output logits and convert to softmax \n",
        "    # Find the classification index with the highest value.\n",
        "    #  \n",
        "    pred_label = np.argmax(tf.nn.softmax(output.logits, axis=-1))\n",
        "\n",
        "    return pred_label\n",
        "\n",
        "# Create a list of unique labels that we will recognize.\n",
        "# Obviously this has to match what we trained our model with\n",
        "# earlier.\n",
        "#\n",
        "sentence_labels = [\n",
        "              \"others\",\n",
        "              \"atis_abbreviation\",\n",
        "              \"atis_aircraft\",\n",
        "              \"atis_airfare\",\n",
        "              \"atis_airline\",\n",
        "              \"atis_flight\",\n",
        "              \"atis_flight_time\",\n",
        "              \"atis_greeting\",\n",
        "              \"atis_ground_service\",\n",
        "              \"atis_quantity\",\n",
        "              \"atis_yes\",\n",
        "              \"atis_no\"]\n",
        "\n",
        "# Load the saved model file\n",
        "#\n",
        "intent_model = TFAutoModelForSequenceClassification.from_pretrained(\"intent_model\")\n",
        "\n",
        "text = input()\n",
        "\n",
        "print (sentence_labels[infer_intent(intent_model, text)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3fouh-hMkEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}