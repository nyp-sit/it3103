{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "token_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week14/token_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXdc1O6Gm-a"
      },
      "source": [
        "# Token Classification (Named Entity Recognition)\n",
        "\n",
        "In this practical we will learn how to use the HuggingFace Transformers library to perform token classification.\n",
        "\n",
        "Just like what we did in Practical 3a, we will use the DistiBERT transformer architecture, which also allows us to classify each and every word in a sentence.\n",
        "\n",
        "####**NOTE: Be sure to set your runtime to a GPU instance!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy827DuG3b7"
      },
      "source": [
        "## Install Transformers\n",
        "\n",
        "Run the following cell to install the HuggingFace Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-D3yf5P0rWz"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apSbg63-PhYf"
      },
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0wg6EKWOMhj"
      },
      "source": [
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/token_train.txt\n",
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/token_test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8paYUgs38H5"
      },
      "source": [
        "## Process the data \n",
        "\n",
        "The data file is in CoNLL format: \n",
        "\n",
        "```\n",
        "sentence1-word1  PPTag-1-1  ChunkTag-1-1  NERTag-1-1\n",
        "sentence1-word2  PPTag-1-2  ChunkTag-1-2  NERTag-1-2\n",
        "sentence1-word3  PPTag-1-3  ChunkTag-1-3  NERTag-1-3\n",
        "<empty line>\n",
        "sentence2-word1  PPTag-2-1  ChunkTag-2-1  NERTag-2-1\n",
        "sentence2-word2  PPTag-2-2  ChunkTag-2-2  NERTag-2-2\n",
        "...\n",
        "sentence2-wordn  PPTag-2-n  ChunkTag-2-n  NERTag-2-n\n",
        "<empty line>\n",
        "...\n",
        "```\n",
        "\n",
        "For example, the sentence \"U.N. official Ekeus heads for Baghdad.\" will be represented as follow in CoNLL format: \n",
        "\n",
        "```\n",
        "U.N.      NNP  I-NP  I-ORG\n",
        "official  NN   I-NP  O\n",
        "Ekeus     NNP  I-NP  I-PER\n",
        "heads     VBZ  I-VP  O\n",
        "for       IN   I-PP  O\n",
        "Baghdad   NNP  I-NP  I-LOC\n",
        ".         .    O     O\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2aiV0haVvI-"
      },
      "source": [
        "We define a function to read the data file line by line and combined lines that belong to a sentence into a list of words and list of tags. \n",
        "\n",
        "As we are only interested in the Named Entity Recognition (NER) tags, we will only extract tags from column_index 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7mWDJnwTy9S"
      },
      "source": [
        "# This function returns a 2D list of words and a 2D list of labels\n",
        "# corresponding to each word.\n",
        "\n",
        "def load_conll(filepath, delimiter=' ', word_column_index=0, label_column_index=3):\n",
        "    all_texts = []\n",
        "    all_tags = []\n",
        "\n",
        "    texts = []\n",
        "    tags = []\n",
        "\n",
        "    # Opens the file.\n",
        "    #\n",
        "    with open(filepath, \"r\") as f:\n",
        "\n",
        "        # Loops through each line \n",
        "        for line in f:\n",
        "\n",
        "            # Split each line by its delimiter (default is a space)\n",
        "            tokens = line.split(delimiter)\n",
        "\n",
        "            # If the line is empty, treat it as the end of the\n",
        "            # previous sentence, and construct a new sentence\n",
        "            #\n",
        "            if len(tokens) == 1:\n",
        "                # Append the sentence\n",
        "                # \n",
        "                all_texts.append(texts)\n",
        "                all_tags.append(tags)\n",
        "\n",
        "                # Create a new sentence\n",
        "                #\n",
        "                texts = []\n",
        "                tags = []\n",
        "            else:\n",
        "                # Not yet end of the sentence, continue to add\n",
        "                # words into the current sentence\n",
        "                #\n",
        "                thistext = tokens[word_column_index].replace('\\n', '')\n",
        "                thistag = tokens[label_column_index].replace('\\n', '')\n",
        "\n",
        "                texts.append(thistext)\n",
        "                tags.append(thistag)\n",
        "\n",
        "    # Insert the last sentence if it contains at least 1 word.\n",
        "    #\n",
        "    if len(texts) > 0:\n",
        "        all_texts.append(texts)\n",
        "        all_tags.append(tags)\n",
        "\n",
        "    # Return the result to the caller\n",
        "    #\n",
        "    return all_texts, all_tags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjtIJjOWWrs"
      },
      "source": [
        "We will now process our files with the function and examine the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgARQPbyWdDh"
      },
      "source": [
        "train_texts, train_tags = load_conll(\"token_train.txt\")\n",
        "val_texts, val_tags = load_conll(\"token_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7O43-HxWlmn"
      },
      "source": [
        "print(train_texts[:3])\n",
        "print(train_tags[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVm7WR1kXm2F"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Now we have our texts and labels. Before we can feed the texts and labels into our model for training, we need to tokenize our texts and also encode our labels into numeric forms.\n",
        "\n",
        "We first define the token labels we need and define the mapping to a numeric index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOghzNYqE5v"
      },
      "source": [
        "# Define a list of unique token labels that we will recognize\n",
        "#\n",
        "token_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Create a reverse-mapping dictionary of the label -> index.\n",
        "#\n",
        "token_labels_id_by_label = {tag: id for id, tag in enumerate(token_labels)}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQNRo7B3EidZ"
      },
      "source": [
        "\n",
        "We will now need to tokenize our text.  Let's look at a potential problem that can happen we do tokenization. Transformers model like Bert uses WordPiece Tokenization, meaning that single words are split into multiple tokens (this is done to solve the out-of-vocabulary problem for rare words). For example, DistilBert’s tokenizer would split the `[\"Nadim\", \"Ladki\"]` into the tokens `[[CLS], \"na\", \"##im\",\"lad\", ##ki\", [SEP]]`. This is a problem for us because we have exactly one tag per token. If the tokenizer splits a token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels.\n",
        "\n",
        "Before tokenization with WordPiece, it is one to one matching between tokens and tags:\n",
        "\n",
        "```\n",
        "tokens = [\"Nadim\", \"Ladki\"]\n",
        "labels = ['B-PER', 'I-PER']\n",
        "```\n",
        "\n",
        "After tokenization with WordPiece, there is no more one-to-one match between them: \n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "labels = ['B-PER', 'I-PER']\n",
        "```\n",
        "\n",
        "One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in Transformers by setting the labels we wish to ignore to -100. We will also ignore special tokens like `[CLS]` and `[SEP]`. In the example above, if the label for 'Nadim' 1 (index for B-PER) and 'Ladki' is 2 (index for I-PER), we would set the labels as follows: \n",
        "\n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "labels = [-100, 1, -100, 2, -100, -100]\n",
        "```\n",
        "\n",
        "But how do we know which token to ignore? This is where we need to use the offset_mapping from the tokenizer. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token’s start position and end position relative to the original token it was split from. \n",
        "\n",
        "For example, in the origial token 'nadim', subtoken \"##dim\" is starts at original position 3 (i.e. `d`) and ends in position 5 (i.e. `m`). So the offset_mapping for `##dim` thus is given as `(3,5)`. Also, you can see that the special tokens like `[CLS]` has a offset_mapping of `(0,0)`. \n",
        "\n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "offset_mappings = [(0, 0), (0, 3), (3, 5), (0, 3), (3, 5), (0, 0)]\n",
        "```\n",
        "\n",
        "That means that if the first position in the tuple is anything other than 0, we will set its corresponding label to -100. While we’re at it, we can also set labels to -100 if the second position of the offset mapping is 0, since this means it must be a special token like `[SEP]` or `[CLS]`.\n",
        "\n",
        "The following function `encode_tags()` takes in original tags and encode it according to the logic described above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyvtz395c0ir"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[token_labels_id_by_label[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT5Wu4sFYorz"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the DistilBERT tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# define a reverse lookup table for mapping id to corresponding word\n",
        "index2word = { value: key for key, value in tokenizer.get_vocab().items() }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJx99lYbMfC6"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, \n",
        "                            is_split_into_words=True, \n",
        "                            return_offsets_mapping=True, \n",
        "                            padding=True, \n",
        "                            truncation=True)\n",
        "val_encodings = tokenizer(val_texts, \n",
        "                          is_split_into_words=True, \n",
        "                          return_offsets_mapping=True, \n",
        "                          padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXImpLVMqzo"
      },
      "source": [
        "Let's examine the encoding of one sample. Since we set `return_offsets_mapping` to `True`, we will see the offset_mapping in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6AVs9LNMngq"
      },
      "source": [
        "print(train_encodings.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS3TixmANXtu"
      },
      "source": [
        "for i in range(5):\n",
        "  print([index2word[id] for id in train_encodings.input_ids[i] if id != 0])\n",
        "  print(train_encodings.offset_mapping[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LuZ9ovPRFDl"
      },
      "source": [
        "Now we will go ahead and encode our tag labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWovfkggRK7a"
      },
      "source": [
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAonFBXU5EWX"
      },
      "source": [
        "Now we are ready to create our datasets for training and evaluating our models. Before that we need to remove offset_mapping from the encodings as it is not needed by our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpwVcZEGpnv9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxisdqSj36C-"
      },
      "source": [
        "Run the following cell below to see the first few samples of the train dataset to see if they looks all right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7FKTq7mEbo"
      },
      "source": [
        "iterator = iter(train_dataset)\n",
        "\n",
        "for i in range(3):\n",
        "    print (train_texts[i])\n",
        "    print(iterator.get_next())\n",
        "    print (\"---\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDGUmBIPfp4"
      },
      "source": [
        "## Train our Token Classification Model\n",
        "\n",
        "We will now set up the training configuration. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOcCiXN-06lx"
      },
      "source": [
        "from transformers import (\n",
        "    TFAutoModelForTokenClassification, \n",
        "    TFTrainer, \n",
        "    TFTrainingArguments\n",
        ")\n",
        "\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    token_model = TFAutoModelForTokenClassification.from_pretrained('distilbert-base-uncased', \n",
        "                                                              num_labels=len(token_labels))\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=token_model,                   # the instantiated Token Classification Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEGEiLiULBM"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxwnjFtc4jZ2"
      },
      "source": [
        "token_model.save_pretrained('./my_tokenmodel/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzcve0wA5A-V"
      },
      "source": [
        "#!zip -r my_model.zip ./my_tokenmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9pUdf68qHn"
      },
      "source": [
        "## Section 8 - Evaluate the Model\n",
        "\n",
        "Run the following cells below to evaluate your model performance.\n",
        "\n",
        "Obviously, you can only do this AFTER your training is completed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqobMwQOGiYP"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForTokenClassification\n",
        ")\n",
        "                          \n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cDPJ8cd_Mkg"
      },
      "source": [
        "model = TFAutoModelForTokenClassification.from_pretrained('my_tokenmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrzfgM8bjJc4"
      },
      "source": [
        "def infer_tokens(text):\n",
        "    encodings = tokenizer([text], is_split_into_words=True, padding=True, truncation=True, return_offsets_mapping=True, return_tensors=\"tf\")\n",
        "\n",
        "    label_mapping = [0] * len(encodings.offset_mapping[0])\n",
        "    for i, offset in enumerate(encodings.offset_mapping[0]):\n",
        "        if encodings.offset_mapping[0][i][0] == 0 and encodings.offset_mapping[0][i][1] != 0:\n",
        "            label_mapping[i] = 1\n",
        "\n",
        "    encodings.pop(\"offset_mapping\")\n",
        "    #encodings = encodings.to(\"cuda\")\n",
        "\n",
        "    # Use the token classification model to predict the labels\n",
        "    # for each word.\n",
        "    #\n",
        "    output = token_model(encodings)[0]\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for i in range(output.shape[1]):\n",
        "        if label_mapping[i] == 1:\n",
        "            result.append(np.argmax(output[0][i]).item())\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmI5r-tY-4y0"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# This function takes in a list of sentences (texts) and passes them into the\n",
        "# infer_tokens method to tokenize and predict each word's label.\n",
        "# \n",
        "# It will then convert the list of labels into their numeric index, and\n",
        "# return both actual label and predicted label to the caller.\n",
        "#\n",
        "def get_actual_pred_y(texts, labels):\n",
        "    all_actual_y = []\n",
        "    all_pred_y = []\n",
        "\n",
        "    for i in tqdm(range(len(texts))):\n",
        "        x = texts[i]\n",
        "\n",
        "        actual_y = list(filter(lambda x: x != -100, labels[i]))\n",
        "        pred_y = infer_tokens(x)\n",
        "\n",
        "        if (len(actual_y) == len(pred_y)):\n",
        "            all_actual_y += actual_y\n",
        "            all_pred_y += pred_y\n",
        "        else:\n",
        "            print (\"Error: %d, %d, %d, %s \" % (i, len(actual_y), len(pred_y), x ))\n",
        "\n",
        "    return all_actual_y, all_pred_y\n",
        "\n",
        "# Get the actual and predicted labels for all words in all sentences\n",
        "# for both the training and the test set.\n",
        "# \n",
        "#actual_y_train, pred_y_train = get_actual_pred_y(train_texts, train_labels)\n",
        "actual_y_test, pred_y_test = get_actual_pred_y(val_texts, val_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5jUinaajcsh"
      },
      "source": [
        "from sklearn.metrics import classification_report \n",
        "\n",
        "print(classification_report(actual_y_test, pred_y_test, target_names=token_labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VabGEW_qVZ2t"
      },
      "source": [
        "Ok, let's test it on your own text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQMQ2DabQHjb"
      },
      "source": [
        "text = input()\n",
        "\n",
        "text = text.split(\" \")\n",
        "\n",
        "print(text)\n",
        "predicted = [token_labels[label] for label in infer_tokens(text)]\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyCgX19HUeeq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}