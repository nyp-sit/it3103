{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week15/token_classification_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXdc1O6Gm-a"
      },
      "source": [
        "# Token Classification (Named Entity Recognition)\n",
        "\n",
        "In this practical we will learn how to use the HuggingFace Transformers library to perform token classification.\n",
        "\n",
        "Just like what we did in previous practical on BERT Transformer, we will use the DistilBERT transformer to classify each and every word (token) in a sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy827DuG3b7"
      },
      "source": [
        "## Install Transformers\n",
        "\n",
        "Run the following cell to install the HuggingFace Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "sUb65UVgLepB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apSbg63-PhYf"
      },
      "source": [
        "## Get the data\n",
        "\n",
        "In this lab, we will use the CoNLL-2003 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0wg6EKWOMhj"
      },
      "outputs": [],
      "source": [
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/conll2003.zip\n",
        "!unzip conll2003.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the data \n",
        "\n",
        "The data file is in CoNLL format: \n",
        "\n",
        "```\n",
        "sentence1-word1  PPTag-1-1  ChunkTag-1-1  NERTag-1-1\n",
        "sentence1-word2  PPTag-1-2  ChunkTag-1-2  NERTag-1-2\n",
        "sentence1-word3  PPTag-1-3  ChunkTag-1-3  NERTag-1-3\n",
        "<empty line>\n",
        "sentence2-word1  PPTag-2-1  ChunkTag-2-1  NERTag-2-1\n",
        "sentence2-word2  PPTag-2-2  ChunkTag-2-2  NERTag-2-2\n",
        "...\n",
        "sentence2-wordn  PPTag-2-n  ChunkTag-2-n  NERTag-2-n\n",
        "<empty line>\n",
        "...\n",
        "```\n",
        "\n",
        "For example, the sentence \"U.N. official Ekeus heads for Baghdad.\" will be represented as follow in CoNLL format: \n",
        "\n",
        "```\n",
        "U.N.      NNP  I-NP  I-ORG\n",
        "official  NN   I-NP  O\n",
        "Ekeus     NNP  I-NP  I-PER\n",
        "heads     VBZ  I-VP  O\n",
        "for       IN   I-PP  O\n",
        "Baghdad   NNP  I-NP  I-LOC\n",
        ".         .    O     O\n",
        "```"
      ],
      "metadata": {
        "id": "BgLdC0e1MjqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to read the data file line by line and combined lines that belong to a sentence into a list of words and list of tags. \n",
        "\n",
        "As we are only interested in the Named Entity Recognition (NER) tags, we will only extract tags from column_index 3."
      ],
      "metadata": {
        "id": "jLL7uGOsMlAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7mWDJnwTy9S"
      },
      "outputs": [],
      "source": [
        "# This function returns a 2D list of words and a 2D list of labels\n",
        "# corresponding to each word.\n",
        "\n",
        "def load_conll(filepath, delimiter=' ', word_column_index=0, label_column_index=3):\n",
        "    all_texts = []\n",
        "    all_tags = []\n",
        "\n",
        "    texts = []\n",
        "    tags = []\n",
        "\n",
        "    # Opens the file.\n",
        "    #\n",
        "    with open(filepath, \"r\") as f:\n",
        "\n",
        "        # Loops through each line \n",
        "        for line in f:\n",
        "\n",
        "            # Split each line by its delimiter (default is a space)\n",
        "            tokens = line.split(delimiter)\n",
        "\n",
        "            # If the line is empty, treat it as the end of the\n",
        "            # previous sentence, and construct a new sentence\n",
        "            #\n",
        "            if len(tokens) == 1:\n",
        "                # Append the sentence\n",
        "                # \n",
        "                all_texts.append(texts)\n",
        "                all_tags.append(tags)\n",
        "\n",
        "                # Create a new sentence\n",
        "                #\n",
        "                texts = []\n",
        "                tags = []\n",
        "            else:\n",
        "                # Not yet end of the sentence, continue to add\n",
        "                # words into the current sentence\n",
        "                #\n",
        "                thistext = tokens[word_column_index].replace('\\n', '')\n",
        "                thistag = tokens[label_column_index].replace('\\n', '')\n",
        "\n",
        "                texts.append(thistext)\n",
        "                tags.append(thistag)\n",
        "\n",
        "    # Insert the last sentence if it contains at least 1 word.\n",
        "    #\n",
        "    if len(texts) > 0:\n",
        "        all_texts.append(texts)\n",
        "        all_tags.append(tags)\n",
        "\n",
        "    # Return the result to the caller\n",
        "    #\n",
        "    return all_texts, all_tags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjtIJjOWWrs"
      },
      "source": [
        "We will now process our files with the function and examine the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgARQPbyWdDh"
      },
      "outputs": [],
      "source": [
        "train_texts, train_tags = load_conll(\"token_train.txt\")\n",
        "val_texts, val_tags = load_conll(\"token_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7O43-HxWlmn"
      },
      "outputs": [],
      "source": [
        "print(train_texts[:3])\n",
        "print(train_tags[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVm7WR1kXm2F"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Now we have our texts and labels. Before we can feed the texts and labels into our model for training, we need to tokenize our texts and also encode our labels into numeric forms.\n",
        "\n",
        "We first define the token labels we need and define the mapping to a numeric index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qOghzNYqE5v"
      },
      "outputs": [],
      "source": [
        "# Define a list of unique token labels that we will recognize\n",
        "#\n",
        "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Define a dictionary to map txt label to numeric label\n",
        "label2id = {label:i for i, label in enumerate(label_names)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also import the tokenizer."
      ],
      "metadata": {
        "id": "ln2Jj3J8UNXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT5Wu4sFYorz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = 'distilbert-base-uncased'\n",
        "# Initialize the DistilBERT tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, is_fast=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we tokenize our texts,  let's look at a potential problem that can happen we do tokenization. Transformers model like Bert or DistilBert uses WordPiece tokenization, meaning that a single word can sometimes be split into multiple tokens (this is done to solve the out-of-vocabulary problem for rare words). For example, DistilBert’s tokenizer would split the `[\"Nadim\", \"Ladki\"]` into the tokens `[[CLS], \"na\", \"##im\",\"lad\", ##ki\", [SEP]]`. This is a problem for us because we have exactly one tag per token in the original dataset. If the tokenizer splits a token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels, as illustrated below:\n",
        "\n",
        "Before tokenization with WordPiece, it is one to one matching between tokens and tags:\n",
        "\n",
        "```\n",
        "tokens = [\"Nadim\", \"Ladki\"]\n",
        "labels = ['B-PER', 'I-PER']\n",
        "```\n",
        "\n",
        "After tokenization with WordPiece, there is no more one-to-one match between them: \n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "labels = ['B-PER', 'I-PER']\n",
        "```\n",
        "\n",
        "One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in Transformers by setting the labels we wish to ignore to -100. We will also ignore special tokens like `[CLS]` and `[SEP]`. In the example above, if the label for 'Nadim' is 1 (index for B-PER) and 'Ladki' is 2 (index for I-PER), we would set the labels as follows: \n",
        "\n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "labels = [-100, 1, -100, 2, -100, -100]\n",
        "```\n",
        "\n",
        "But how do we know which are sub-tokens and the special tokens to ignore? Fortunately, the Huggingface tokenize provides us a way to do it: `word_ids`. `word_ids` will tell us which word each token comes from, as well as which words are special tokens (e.g. `[CLS]`). \n",
        "\n",
        "For example, the `word_ids` for the following tokens will be: \n",
        "\n",
        "```\n",
        "tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n",
        "word_ids = [None, 0, 0, 1, 1, None]\n",
        "```\n",
        "\n",
        "`None` means it is a special token. You can see that `\"nad\"`, `\"##im\"` are both having word_ids `0`, which means both tokens comes from the word at index 0, i.e. `\"nadim\"`. Similarly, `\"lad\"` and `\"##ki\"` have word_ids of `1`, which means both comes from the 2nd word, i.e. word at index 1.\n",
        "\n",
        "\n",
        "So we can simply use the following logic to decide how to label each of the processed tokens (i.e tokens that have already processed by the tokenizer, and consist of special tokens and subtokens):\n",
        "- if a token has a `word_id` of `None`, we will set its corresponding label to `-100`. - if the `word_id` of the token appears the 1st time, i.e. different from previous `word_id`, set the label of the token to the corresponding original label. \n",
        "- if the `word_id` is the same as previous `word_id`, set the label for the tokens to `-100`\n",
        "\n",
        "The following function `tokenize_and_align_labels()` takes in original tags and encode it according to the logic described above. \n",
        "\n",
        "Note that for this version of HuggingFace, we need to supply the label as part of the dictionary. That is why we create additional entry `tokenized_inputs['labels']` to hold the labels. "
      ],
      "metadata": {
        "id": "pd9-dD_dPBaH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VPX7Gl5LAcf"
      },
      "outputs": [],
      "source": [
        "# set the max sequence length and require padding\n",
        "max_length=128\n",
        "padding=True\n",
        "\n",
        "def tokenize_and_align_labels(texts, all_tags):\n",
        "    \n",
        "    tokenized_inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=max_length,\n",
        "        padding=padding,\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    for i, tags in enumerate(all_tags):\n",
        "        word_ids = tokenized_inputs[i].word_ids\n",
        "        tokens = tokenized_inputs[i].ids\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "       \n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(int(label2id[tags[word_idx]]))\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "                \n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "        \n",
        "        tokenized_inputs['labels'] = labels\n",
        "        \n",
        "    return tokenized_inputs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoQ1PY2ZLAcf"
      },
      "outputs": [],
      "source": [
        "train_encodings, train_labels = tokenize_and_align_labels(train_texts, train_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUoyXw3TLAcg"
      },
      "outputs": [],
      "source": [
        "val_encodings, val_labels = tokenize_and_align_labels(val_texts, val_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpwVcZEGpnv9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxisdqSj36C-"
      },
      "source": [
        "Run the following cell below to see the first few samples of the train dataset to see if they looks all right."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(train_dataset)\n",
        "\n",
        "for i in range(3):\n",
        "    print (train_texts[i])\n",
        "    print(iterator.get_next())\n",
        "    print (\"---\")"
      ],
      "metadata": {
        "id": "xAUJa6Y4WlMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDGUmBIPfp4"
      },
      "source": [
        "## Train our Token Classification Model\n",
        "\n",
        "We will now load the pretrained model and configure the required token labels for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOcCiXN-06lx"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForTokenClassification, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint, num_labels=len(label_names))\n",
        "\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YUbK5vVHM8m"
      },
      "source": [
        "Let’s double-check that our model has the right number of labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsS2UUWmHB_A"
      },
      "outputs": [],
      "source": [
        "model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTKNwOxZGDle"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "num_epochs = 1\n",
        "num_train_steps = len(train_dataset) * num_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "\n",
        "def dummy_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_pred)\n",
        "\n",
        "losses = {\"loss\": dummy_loss}\n",
        "model.compile(loss=losses,optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbrA53lcH59z"
      },
      "source": [
        "Huggingface model can actually compute loss internally — if you compile without a loss and supply your labels in the input dictionary (as we do in our datasets), then the model will train using that internal loss, which will be appropriate for the task and model type you have chosen.  This works previously using previous version of Tensorflow, but Tensorflow 2.7 seems to require a loss function to be supplied. So we create a dummy loss function as a work-around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJEGEiLiULBM"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "The traditional framework used to evaluate token classification prediction is seqeval. To use this metric, we first need to install the seqeval library:"
      ],
      "metadata": {
        "id": "mUZf2U9jZAq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "0QtA9FebYz5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urzz3jP5LAcr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('seqeval')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following codes, we use our model to predict our val_dataset, in batches. For each batch of tf dataset, we have two parts: 1st contains the `input_ids`, `attention_masks`, and `labels`, while the second one is target label. We will only use the 1st part for prediction, i.e. `batch[0]`\n",
        "\n",
        "Also while looping through the list of predicted label for each token, we will ignore those positions that is labeled \"-100\". "
      ],
      "metadata": {
        "id": "8jLt2o5EZZgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSAWBeXZLAcr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in val_dataset:\n",
        "    logits = model.predict(batch[0])[\"logits\"]\n",
        "    labels = batch[0][\"labels\"]\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    for prediction, label in zip(predictions, labels):\n",
        "        for predicted_idx, label_idx in zip(prediction, label):\n",
        "            if label_idx == -100:\n",
        "                continue\n",
        "            all_predictions.append(label_names[predicted_idx])\n",
        "            all_labels.append(label_names[label_idx])\n",
        "\n",
        "metric.compute(predictions=[all_predictions], references=[all_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxwnjFtc4jZ2"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('my_tokenmodel')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on your sentence\n"
      ],
      "metadata": {
        "id": "Cjzql4D_byUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_tokens(text, model, tokenizer):\n",
        "    # here we assume the text has not been splitted into individual words\n",
        "    text = text.split()\n",
        "    \n",
        "    encodings = tokenizer(\n",
        "        [text],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors='tf')\n",
        "    \n",
        "    logits = model(encodings)[0] # assume only a single prediction\n",
        "    preds = np.argmax(logits, axis=-1)[0]\n",
        "\n",
        "    # as the prediction is on individual tokens, including subtokens, \n",
        "    # we need to group subtokens belonging to the same word together\n",
        "    # again, we use the word_ids to help us here\n",
        "    previous_word_idx = None\n",
        "    word_ids = encodings[0].word_ids\n",
        "    labels = []\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        # we check if the word_id different from previous one, then it is a new word\n",
        "        # we also need to check if the word_id is not None so that we won't include it\n",
        "        if word_idx != previous_word_idx and word_idx != None:\n",
        "            labels.append(label_names[preds[i]])\n",
        "        # update the previous_word_idx to current word_id\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return text, labels"
      ],
      "metadata": {
        "id": "Bb0SneGlb3f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbR23gjFLAcs"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = TFAutoModelForTokenClassification.from_pretrained('my_tokenmodel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ZMzkkNLAcv"
      },
      "outputs": [],
      "source": [
        "sample_text = 'Ashish Vaswani has developed the transformer architecture during his time at Google.'\n",
        "infer_tokens(sample_text, model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "token_classification_v2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}