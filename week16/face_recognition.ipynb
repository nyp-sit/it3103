{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week16/face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1Pj_BMa6U0B"
   },
   "source": [
    "# Facial Recognition\n",
    "\n",
    "In this practical, we will learn how to use existing libraries to implement our own Facial Recognition system.\n",
    "\n",
    "This notebook uses Colab's library to take picture using the webcam and thus only runs in Colab environment and cannot be run on your desktop's Jupyter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u37N7Ie7WCa"
   },
   "source": [
    "## Section 1 - Installing Necessary Libraries\n",
    "\n",
    "Run the following cell below to install the latest MTCNN library.\n",
    "\n",
    "The MTCNN library is a Python library that uses the Multi-Task Cascading Convolutional Neural Networks used to detect faces in an image. The Keras implementation can be found here with pre-trained weights can be found here: https://github.com/ipazc/mtcnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iLzJaOirxl0"
   },
   "outputs": [],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YVcrkObHSGD"
   },
   "source": [
    "## Section 2 - Get the data and model\n",
    "\n",
    "We will download the necessary data and also the pretrained model we need. We will unzip the data into the following folders:\n",
    "- data (this contains the photos of people we want to register into face recognition system\n",
    "- samples (this contains some test images to test our face recognition system)\n",
    "- models (this contains the pretrained FaceNet keras weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlHTqzRPxW01"
   },
   "outputs": [],
   "source": [
    "!wget -q https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/facerecog/data.zip && unzip -qo data.zip  \n",
    "!wget -q https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/facerecog/samples.zip && unzip -qo samples.zip\n",
    "!wget -q https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/facerecog/models.zip && unzip -qo models.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk5by8v_7m-d"
   },
   "source": [
    "## Section 3 - Declare a List of Functions \n",
    "\n",
    "First we define the functions that we will be using later on to process and visualize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uewvppe-vTvi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "from google.colab.patches import cv2_imshow\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Loads an image from a file using OpenCV.\n",
    "# NOTE: OpenCV loads an image in BGR format by default,\n",
    "#       so we must convert it back to the RGB format.\n",
    "def load_image(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Draw a bounding box over the image with a \n",
    "# text.\n",
    "def draw_box(img, x1, y1, x2, y2, text): \n",
    "    img = cv2.rectangle(img,(x1,y1),(x2,y2),(255,255,0),2)\n",
    "\n",
    "    if text != \"\":\n",
    "        img = cv2.rectangle(img,(x1,y1),(x2,y1 + 12),(255,255,0),-1)\n",
    "        img = cv2.putText(img, text, (x1, y1 + 10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Crops out parts of an image based on a list of bounding\n",
    "# boxes. The cropped faces are also resized to 160x160 in\n",
    "# preparation for passing it to FaceNet to compute the\n",
    "# face embeddings.\n",
    "def crop_faces_to_160x160(img, bounding_boxes): \n",
    "    cropped_faces = []\n",
    "\n",
    "    for (x,y,w,h) in bounding_boxes:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "        cropped_face = cv2.resize(cropped_face, (160, 160), interpolation=cv2.INTER_CUBIC)\n",
    "        cropped_faces.append(cropped_face)\n",
    "    \n",
    "    return np.array(cropped_faces)\n",
    "\n",
    "\n",
    "# Shows an image in Colab\n",
    "def show_image(img):\n",
    "    cv2_imshow(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyUpFV9pPVLL"
   },
   "source": [
    "Next we create a function that allows us to take photo using your PC's webcam, using the Colab's provided javascript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckMFXbxKPTxP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename\n",
    "\n",
    "\n",
    "def launch_camera(prompt, filename):\n",
    "\n",
    "    print (prompt)\n",
    "    try:\n",
    "      filename1 = take_photo(filename)\n",
    "      print('Saved to ' + filename1)\n",
    "      \n",
    "      # Show the image which was just taken.\n",
    "      #show_image(filename1)\n",
    "\n",
    "    except Exception as err:\n",
    "      # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "      # grant the page permission to access it.\n",
    "      print(str(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhhFTgpkIjb3"
   },
   "source": [
    "## Section 4 - Detect Faces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ndfDDOUHg2a"
   },
   "source": [
    "We will load the MTCNN library and use it to detect faces in an RGB image of any size.\n",
    "\n",
    "In the detect_faces_with_cnn function, we call MTCNN to detect faces and draw bounding boxes in an image: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0Hq8JgS84Mq"
   },
   "outputs": [],
   "source": [
    "face_detector_mtcnn = MTCNN()\n",
    "\n",
    "# Use MTCNN to detect face bounding boxes. The bounding boxes\n",
    "# returned from this function will be in the following format:\n",
    "# [\n",
    "#    (x, y, w, h), \n",
    "#    (x, y, w, h), \n",
    "#    ...\n",
    "# ]\n",
    "# \n",
    "def detect_faces_with_mtcnn(img):\n",
    "\n",
    "    # Call the face_detector_mtcnn's detect_faces_with_mtcnn function.\n",
    "    # Then, extract only the bounding boxes and return the results\n",
    "    # to the caller as described in the format above.\n",
    "    #\n",
    "    bounding_boxes = []\n",
    "    detected_faces = face_detector_mtcnn.detect_faces(img)\n",
    "    for detected_face in detected_faces:\n",
    "        bounding_boxes.append(detected_face[\"box\"])\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q4ppomKvp7q"
   },
   "source": [
    "## Section 5 - Face Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLDfzNRvHvmw"
   },
   "source": [
    "The FaceNet implementation in Keras with the pre-trained weights on the Microsoft 1 Million Celeb dataset can be found at: https://github.com/nyoki-mtl/keras-facenet. We will first download the FaceNet model.\n",
    "\n",
    "We will then load the FaceNet model and use it to extract the face embeddings.\n",
    "\n",
    "1. Load up a pre-trained FaceNet model:\n",
    "2. Call the FaceNet model to retrieve our face embeddings:\n",
    "\n",
    "\n",
    "*NOTE: that before you can pass in the RGB image, the RGB values in the image has to be readjusted from 0 - 255 to -1.0 to 1.0. That is the purpose of the formula (x - 128) / 128.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCym7FcJvudw"
   },
   "outputs": [],
   "source": [
    "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/facerecog/models/facenet_keras.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZOBWQlN8-DQ"
   },
   "outputs": [],
   "source": [
    "# Load the FaceNet's pre-trained face embedding model.\n",
    "face_embedding_facenet = keras.models.load_model('facenet_keras.h5')\n",
    "\n",
    "# Gets a list of face embeddings from FaceNet for each cropped face.\n",
    "# \n",
    "# The cropped_faces parameter is a numpy array of Nx160x160x3,\n",
    "# where N is any number of faces cropped from an image.\n",
    "def get_face_embeddings_with_facenet(cropped_faces):\n",
    "\n",
    "    # To get the embeddings, first, normalize the RGB values from \n",
    "    #    0 to 255 => -1.0 to 1.0 \n",
    "    #\n",
    "    # Then pass the result into the face_embedding_facenet's predict\n",
    "    # model and return the results (Nx128 embeddings) as is.\n",
    "    #...#\n",
    "    cropped_faces = (cropped_faces.astype(\"float32\") - 128) / 128\n",
    "    return face_embedding_facenet.predict(cropped_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfTkYeDOIeCE"
   },
   "source": [
    "Run the following cell to test and see if the `detect_faces_with_mtcc()` and the `get_face_embeddings_with_facenet()` functions are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Did6Bsgi5oVv"
   },
   "outputs": [],
   "source": [
    "# Loads an image for testing\n",
    "img = load_image('samples/test.jpg')\n",
    "\n",
    "# Find all detected faces\n",
    "bounding_boxes = detect_faces_with_mtcnn(img)\n",
    "\n",
    "# Draws bounding boxes for the faces\n",
    "for (x,y,w,h) in bounding_boxes:\n",
    "    img = draw_box(img, x, y, x+w, y+h, \"test\")\n",
    "\n",
    "# Show the image with the bounding boxes.\n",
    "show_image(img)\n",
    "\n",
    "# Crop the faces\n",
    "cropped_faces = crop_faces_to_160x160(img, bounding_boxes)\n",
    "\n",
    "# Print the shape of the face embeddings for each cropped face. \n",
    "# You will see that the embedding is a vector of 128 numbers\n",
    "print(get_face_embeddings_with_facenet(cropped_faces).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz6YsVZA6QWO"
   },
   "source": [
    "## Section 6 - Onboarding\n",
    "\n",
    "If we want to recognise faces, we must have a database of face images (with the person's name) first. \n",
    "\n",
    "In this section, assume that we only have 3 people we want to recognise: Satya Nadella (Microsoft's CEO), Steve Jobs (Apple's Founder), and Tim Cook (Apple's CEO), and we only have 1 photo of each person. \n",
    "\n",
    "We will build a database (to simplify the codes, we just use a dictionary to represent the database), that contains the name of the person and the person's photo. However, to speed up processing time and save storage space, we will pre-extract the person's face embedding and store the embedding in the database instead of the original photo. \n",
    "\n",
    "```\n",
    "{ 'satya_nadella' : [ [0.1, 0.2, ..., 0.01] ], \n",
    "  'tim_cook' : [ [0.01, 0.03, 0.01, ..., 0.01 ] ]\n",
    "  'steve_jobs' : [ [0.03, 0.1, 0.2, ..., 0.3] ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD-6Uk98NKX_"
   },
   "source": [
    "The profile photo of each person in its own subdirectory under the directory `data`. We go through each photo in the subdirectories and extract the face embeddings and store the numpy array on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlRGDwH65Sl2"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "db_folder = 'data'\n",
    "\n",
    "for dirname in os.listdir(db_folder):\n",
    "    fpath = os.path.join(db_folder, dirname)\n",
    "    if os.path.isdir(fpath):\n",
    "        print(f\"{fpath} is dir\")\n",
    "        for filename in os.listdir(fpath):\n",
    "            fpath2 = os.path.join(fpath, filename)\n",
    "            if os.path.isfile(fpath2) and fpath2.endswith('.jpg'):\n",
    "                print(fpath2)\n",
    "                img = load_image(fpath2)\n",
    "                face_boxes = detect_faces_with_mtcnn(img)\n",
    "                \n",
    "                for (x,y,w,h) in face_boxes:\n",
    "                    img = draw_box(img, x, y, x+w, y+h, \"dirname\")\n",
    "                    show_image(img)\n",
    "                    cropped_faces = crop_faces_to_160x160(img, face_boxes)\n",
    "                    if (cropped_faces.shape[0] > 0):\n",
    "                        # assume we only have 1 cropped face, save only the 1st embedding \n",
    "                        embedding = get_face_embeddings_with_facenet(cropped_faces)[0]\n",
    "                        fobj = os.path.join(fpath, filename)\n",
    "                        print(f\"saving as {fobj}\")\n",
    "                        np.save(fobj, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B6zDcOwUXeo"
   },
   "source": [
    "We will now create our dictionary (database) of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RApWoN_Ac_f_"
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "lookup_tb = {}\n",
    "\n",
    "for dir in os.listdir(db_folder):\n",
    "    fpath = os.path.join(db_folder, dir)\n",
    "    if os.path.isdir(fpath):\n",
    "        #print(dir)\n",
    "        if dir not in lookup_tb:\n",
    "            lookup_tb[dir] = []\n",
    "        matches = fnmatch.filter(os.listdir(fpath), '*.npy')\n",
    "        for match in matches: \n",
    "            f = os.path.join(fpath, match)\n",
    "            e = np.load(f)\n",
    "            lookup_tb[dir].append(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXxrT7lkwWHF"
   },
   "source": [
    "## Section 7 - Performing Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSM0zo8-Vxek"
   },
   "source": [
    "In order to determine the identity of a person, we need to see how similar the face embedding of a person to be recognised, is to any face embedding in our database. The face embedding in our database that has highest similarity with the face embedding to be recognised will be the person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6qGNu0UWfDq"
   },
   "source": [
    "Let's define our simlarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4haHq-iWok0"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# Cosine similarity is defined by the formula:\n",
    "#                    A . B\n",
    "#     similarity = ---------\n",
    "#                   |A| |B|\n",
    "#  \n",
    "def compute_similarity(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "    return cos_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6z_n2EjqW7UZ"
   },
   "outputs": [],
   "source": [
    "def recognize_face(img, threshold=0.6, show_cropped_face=False): \n",
    "    \n",
    "    face_boxes = detect_faces_with_mtcnn(img)\n",
    "\n",
    "    cropped_faces = crop_faces_to_160x160(img, face_boxes)\n",
    "    if cropped_faces.shape[0] == 0:\n",
    "        return\n",
    "\n",
    "    # Extract the embeddings of all cropped faces\n",
    "    embeddings_to_be_recognised = get_face_embeddings_with_facenet(cropped_faces)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(0, len(cropped_faces)): \n",
    "        if show_cropped_face: \n",
    "            show_image(cropped_faces[i])\n",
    "\n",
    "        # check our database \n",
    "        max_similarity = -np.inf\n",
    "        recognized_name = '???'\n",
    "        for name, embedding in lookup_tb.items():\n",
    "            # compute similarity\n",
    "            similarity = compute_similarity(embedding[0], embeddings_to_be_recognised[i])\n",
    "    \n",
    "            if similarity > max_similarity: \n",
    "                max_similarity = similarity\n",
    "                recognized_name = name\n",
    "        \n",
    "        if max_similarity >= threshold: \n",
    "            result.append( { 'box': face_boxes[i], 'face_name': recognized_name } )\n",
    "        else: \n",
    "            result.append( { 'box': face_boxes[i], 'face_name': '???'} )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50a1NcYH9AUc"
   },
   "source": [
    "In this next cell, update the codes to call the recognize_face function and draw boxes around the faces in the image with the recognized name.\n",
    "\n",
    "A practical application will use and process this information for other requirements. \n",
    "\n",
    "For example, if this is photograph from a CCTV, you may want to track that the person was recognized to be standing at a certain location and a specific date/time this image was captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZhZT4q59-8k"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 8):\n",
    "    print (\"--- photo%d.jpg ---\" % (i))\n",
    "\n",
    "    filename = './samples/photo%d.jpg' % (i)\n",
    "    img = load_image(filename)\n",
    "    results = recognize_face(img, show_cropped_face=False)\n",
    "\n",
    "    for r in results:\n",
    "        #print(r)\n",
    "        draw_box(img, r[\"box\"][0], r[\"box\"][1], r[\"box\"][0] + r[\"box\"][2], r[\"box\"][1] + r[\"box\"][3], r[\"face_name\"] )\n",
    "\n",
    "    show_image(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKYBpj9r9sy5"
   },
   "source": [
    "## Section 8 - Try it on your own face! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWLwvFNhOIvN"
   },
   "source": [
    "Let's write a convenient function to perform both detection and extract face embedding, assuming only one face per picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16euxxsRNM62"
   },
   "outputs": [],
   "source": [
    "def get_embedding_from_photo(filename):\n",
    "\n",
    "    # Load the image.\n",
    "    img = load_image(filename)\n",
    "\n",
    "    # Detect faces and extract all bounding boxes\n",
    "    bounding_boxes = detect_faces_with_mtcnn(img)\n",
    "\n",
    "    # Crop out the faces from the image\n",
    "    cropped_faces = crop_faces_to_160x160(img, bounding_boxes)\n",
    "\n",
    "    if cropped_faces.shape[0] == 0:\n",
    "        return\n",
    "    \n",
    "    # Take the image of only the first detected face\n",
    "    cropped_face = cropped_faces[0:1, :, :, :]\n",
    "\n",
    "    # Show the cropped out face\n",
    "    show_image(cropped_face[0])\n",
    "\n",
    "    # Get the face embeddings using FaceNet and return the results.\n",
    "    return get_face_embeddings_with_facenet(cropped_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmtT2EPrGf98"
   },
   "source": [
    "Run the following cell to launch the camera in Colab to take a picture. \n",
    "\n",
    "We want to simulate using the photograph stored in an access card, passport of an identity card. So you can use any of of identification card that contains your photo.\n",
    "\n",
    "**NOTE: Your photograph is saved into Google Colab. If you are not comfortable with this, please use a photograph of another person that you can display on your mobile phone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQD73Lhd2IZU"
   },
   "outputs": [],
   "source": [
    "launch_camera(\"Take a photo of your identification card\", \"webcam01.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_flfVDGPk4e"
   },
   "outputs": [],
   "source": [
    "print('Enter your name:')\n",
    "name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikw5FXwNMtFn"
   },
   "outputs": [],
   "source": [
    "embedding = get_embedding_from_photo('webcam01.jpg')\n",
    "lookup_tb[name] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_xBEdblE_qa"
   },
   "source": [
    "Now, launch the camera again, to take another photograph of your live self.\n",
    "\n",
    "**NOTE: Your photograph is saved into Google Colab. If you are not comfortable with this, please use a photograph of another person that you can display on your mobile phone**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MzI-ANk-T2s"
   },
   "outputs": [],
   "source": [
    "launch_camera(\"Take a photo of yourself in the live camera\", \"webcam02.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH9ozd3HQZYQ"
   },
   "outputs": [],
   "source": [
    "img = load_image('webcam02.jpg')\n",
    "r = recognize_face(img, show_cropped_face=False)[0]\n",
    "draw_box(img, r[\"box\"][0], r[\"box\"][1], r[\"box\"][0] + r[\"box\"][2], r[\"box\"][1] + r[\"box\"][3], r[\"face_name\"] )\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5X3O2lVjGk-Y"
   },
   "source": [
    "Finally run the following cell to see the cosine similarity score between the 2 photographs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59HNedGm_Zys"
   },
   "outputs": [],
   "source": [
    "e1 = get_embedding_from_photo('webcam01.jpg')\n",
    "e2 = get_embedding_from_photo('webcam02.jpg')\n",
    "\n",
    "# convert 2D into 1D vector\n",
    "similarity = compute_similarity(e1[0], e2[0])\n",
    "\n",
    "print (\"Cosine Similarity : %f\" % similarity)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "facial_recognition_and_verification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
