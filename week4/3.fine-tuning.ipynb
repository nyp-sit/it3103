{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week4/3.fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce6b8183-8ef0-4664-813f-6a635bda0f15",
      "metadata": {
        "id": "ce6b8183-8ef0-4664-813f-6a635bda0f15"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Another widely used transfer learning technique is _fine-tuning_. \n",
        "Fine-tuning involves unfreezing a few of the top layers \n",
        "of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in our case, the \n",
        "fully-connected classifier) and these unfrozen top layers. This is called \"fine-tuning\" because it slightly adjusts the more abstract \n",
        "representations of the model being reused, in order to make them more relevant for the problem at hand.\n",
        "\n",
        "![fine-tuning VGG16](https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/resources/vgg16_fine_tuning.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768ead86-19a3-4908-be5b-57538c555a19",
      "metadata": {
        "id": "768ead86-19a3-4908-be5b-57538c555a19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "import keras.applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790141c4-2bee-4431-8449-204417495a64",
      "metadata": {
        "id": "790141c4-2bee-4431-8449-204417495a64"
      },
      "source": [
        "It was necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized \n",
        "classifier on top. For the same reason, it is only possible to fine-tune the top layers of the convolutional base once the classifier on \n",
        "top has already been trained. If the classified wasn't already trained, then the error signal propagating through the network during \n",
        "training would be too large, and the representations previously learned by the layers being fine-tuned would be destroyed. Thus the steps \n",
        "for fine-tuning a network are as follow:\n",
        "\n",
        "1. Add your custom network on top of an already trained base network.\n",
        "2. Freeze the base network.\n",
        "3. Train the part you added.\n",
        "4. Unfreeze some layers in the base network.\n",
        "5. Jointly train both these layers and the part you added.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RB9VSinUyvhV",
      "metadata": {
        "id": "RB9VSinUyvhV"
      },
      "outputs": [],
      "source": [
        "img_height, img_width = 128, 128\n",
        "\n",
        "# Load the pre-trained model \n",
        "base_model = keras.applications.VGG16(input_shape=(img_height, img_width) + (3,),\n",
        "                                         include_top=False,\n",
        "                                         weights='imagenet')\n",
        "\n",
        "preprocess_input_fn = keras.applications.vgg16.preprocess_input\n",
        "\n",
        "# freeze the base layer \n",
        "base_model.trainable = False\n",
        "\n",
        "# Add input layer \n",
        "inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
        "# Add preprocessing layer\n",
        "x = preprocess_input_fn(inputs)\n",
        "# Add the base, set training to false to freeze the convolutional base\n",
        "x = base_model(x)\n",
        "# Add our classification head\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "x = keras.layers.Dense(units=512, activation=\"relu\")(x)\n",
        "x = keras.layers.Dropout(rate=0.5)(x)\n",
        "outputs = keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "base_learning_rate = 0.001\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", \n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=base_learning_rate), \n",
        "                  metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0UVNujET2sho",
      "metadata": {
        "id": "0UVNujET2sho"
      },
      "source": [
        "Let's confirm all the layers of convolutional base are frozen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NX2qOF-52eVq",
      "metadata": {
        "id": "NX2qOF-52eVq"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    print(layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q418z8cI3V-O",
      "metadata": {
        "id": "Q418z8cI3V-O"
      },
      "source": [
        "Let's print out the model summary and see how many trainable weights. We can see that we only 263,169 trainable weights (parameters), coming from the classification head that put on top of the convolutional base. (For comparison, a VGG16 has total of 14,714,688 weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZkwKVe2_2_qF",
      "metadata": {
        "id": "ZkwKVe2_2_qF"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "toPgcpoW4HIm",
      "metadata": {
        "id": "toPgcpoW4HIm"
      },
      "source": [
        "## Creating Datasets\n",
        "\n",
        "We will setup our training and validation dataset as we did in earlier exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8d2e8a-b80a-410a-afb7-b7534a83af77",
      "metadata": {
        "id": "dc8d2e8a-b80a-410a-afb7-b7534a83af77"
      },
      "outputs": [],
      "source": [
        "dataset_URL = 'https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_subset.tar.gz'\n",
        "path_to_zip = keras.utils.get_file('cats_and_dogs_subset.tar.gz', origin=dataset_URL, extract=True, cache_dir='.')\n",
        "dataset_dir = os.path.join(os.path.dirname(path_to_zip), \"cats_and_dogs_subset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208febe1-3fdf-41e3-999c-3fac7e70f007",
      "metadata": {
        "id": "208febe1-3fdf-41e3-999c-3fac7e70f007"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "image_size = (img_height, img_width)\n",
        "\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")\n",
        "val_ds = keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZSyhl8et47AV",
      "metadata": {
        "id": "ZSyhl8et47AV"
      },
      "source": [
        "## Train the classification head \n",
        "\n",
        "We will go ahead and train our classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cqSDQ4gP5HEO",
      "metadata": {
        "id": "cqSDQ4gP5HEO"
      },
      "outputs": [],
      "source": [
        "# create model checkpoint callback to save the best model checkpoint\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_checkpoint\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, \n",
        "          epochs=30, callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172d426c-7666-47b1-84a6-485826b79641",
      "metadata": {
        "id": "172d426c-7666-47b1-84a6-485826b79641"
      },
      "source": [
        "Now we have our classification layers trained, let's start to unfreeze some top layers of the convolutional base to fine tune the weights. \n",
        "We will fine-tune the last 3 convolutional layers, which means that all layers up until `block4_pool` should be frozen, and the layers \n",
        "`block5_conv1`, `block5_conv2` and `block5_conv3` should be trainable.\n",
        "\n",
        "Why not fine-tune more layers? Why not fine-tune the entire convolutional base? We could. However, we need to consider that:\n",
        "\n",
        "* Earlier layers in the convolutional base encode more generic, reusable features, while layers higher up encode more specialized features. It is \n",
        "more useful to fine-tune the more specialized features, as these are the ones that need to be repurposed on our new problem. There would \n",
        "be fast-decreasing returns in fine-tuning lower layers.\n",
        "* The more parameters we are training, the more we are at risk of overfitting. The convolutional base has 15M parameters, so it would be \n",
        "risky to attempt to train it on our small dataset.\n",
        "\n",
        "Thus, in our situation, it is a good strategy to only fine-tune the top 2 to 3 layers in the convolutional base.\n",
        "\n",
        "Let's set this up, we will unfreeze our `base_model`, \n",
        "and then freeze individual layers inside of it, except the last 3 layers. \n",
        "\n",
        "Do a model ``summary()`` and you will see now that the number of trainable weights are now 7,079,424 (around 7 millions), much less than previously, because all the layers are frozen except the last 3 layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690283a8-4c53-4293-b8f2-c0685d8031aa",
      "metadata": {
        "id": "690283a8-4c53-4293-b8f2-c0685d8031aa"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wWXyo3n5903l",
      "metadata": {
        "id": "wWXyo3n5903l"
      },
      "source": [
        "Let us examine model summary again. We can see now that we have more trainable weights 7,342,593 compared to previously 263,169."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8uq86nMv-HxP",
      "metadata": {
        "id": "8uq86nMv-HxP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdZPk1Ay80r_",
      "metadata": {
        "id": "bdZPk1Ay80r_"
      },
      "source": [
        "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage as we do not want to make too drastic changes to the weights in the convolutional layers under fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc1da460-3c8a-40f3-a9e5-14c7b832ef79",
      "metadata": {
        "id": "bc1da460-3c8a-40f3-a9e5-14c7b832ef79"
      },
      "outputs": [],
      "source": [
        "finetune_learning_rate = base_learning_rate / 10.\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=finetune_learning_rate),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=15,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2496b41f-63bd-42ca-aa6a-923a6588d4d8",
      "metadata": {
        "id": "2496b41f-63bd-42ca-aa6a-923a6588d4d8"
      },
      "outputs": [],
      "source": [
        "model.load_weights('best_checkpoint')\n",
        "model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RKt9lTt3_qYi",
      "metadata": {
        "id": "RKt9lTt3_qYi"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "1. Is our fine-tuned model performing better or worse? \n",
        "2. Try to unfreeze less/more layers and see if the model performance improves.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3.fine-tuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}